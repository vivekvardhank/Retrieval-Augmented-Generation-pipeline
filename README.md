# Retrieval-Augmented Generation 

This repo walks through building a complete **Retrieval-Augmented Generation (RAG)** pipeline using [LangChain](https://www.langchain.com/), [FAISS](https://github.com/facebookresearch/faiss), and OpenAI models.

It covers the full flow:

1. **Ingestion** – load and chunk data
2. **Retrieval** – store and query with vector search
3. **Generation** – use a large language model (LLM) to answer questions grounded in retrieved context

---

## Why this matters

Large language models are powerful, but they don’t know everything. With RAG, you can give them access to your own data so they answer questions **accurately and with references**.

This repo shows you how to build that pipeline step by step, in a Jupyter notebook.

---

## What’s inside

* Load documents from a web page with `WebBaseLoader`
* Split text into overlapping chunks with `RecursiveCharacterTextSplitter`
* Create vector embeddings using OpenAI
* Store vectors in FAISS for similarity search
* Use a retriever to fetch relevant chunks
* Pass them into an LLM (GPT-4o) with a prompt template
* Get grounded, context-aware answers back

---

## Setup

1. Clone the repo:

   ```bash
   git clone https://github.com/vivekvardhank/Retrieval-Augmented-Generation-pipeline
   cd Retrieval-Augmented-Generation-pipeline
   ```

2. Create a virtual environment and install dependencies:

   ```bash
   pip install -r requirements.txt
   ```

3. Set up your environment variables in a `.env` file:

   ```
   OPENAI_API_KEY=your_openai_api_key
   LANGCHAIN_API_KEY=your_langchain_api_key
   LANGCHAIN_PROJECT=your_project_name
   ```

---

## How to run

Open the Jupyter notebook:

```bash
jupyter notebook basicapp.ipynb
```

Run through the cells to:

* Ingest documents
* Build embeddings and a FAISS vector store
* Run similarity search
* Query the retriever with natural language
* Get answers generated by the LLM

---

## Example Query

**Input**:

```
are llms non-deterministic?
```

**Output (sample)**:

```
Yes, LLMs are non-deterministic. Their outputs can vary across runs because of factors like sampling strategies and randomness during generation.
```

---

## Next Steps

This notebook gives you an end-to-end working RAG pipeline. You can take it further by:

* Ingesting PDFs, Notion docs, or other data sources
* Trying different embedding models
* Using a managed vector DB (like Pinecone, Weaviate, or Chroma)
* Wrapping the pipeline in a Streamlit or Gradio UI for an interactive chatbot

